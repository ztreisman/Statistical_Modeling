---
title: "Regression and ANOVA"
author: "Zack Treisman"
date: "Spring 2021"
output: 
  beamer_presentation: 
    includes: 
      in_header: ../slide_style.tex
bibliography: ../bibliography.bib    
citecolor: blue
---

```{r setup, include=FALSE}
knitr::opts_knit$set(global.par = TRUE)

library(emdbook)
library(ggplot2)
library(car)

```

```{r,include=FALSE}
par(mar = c(4, 4, 0.5, 0.5)) # Set margins
``` 


## Philosophy

**Regression** generally refers to a family of techniques where a model $\hat{y}=f(x)$ is fit to data $(x_i, y_i)$ such that the **mean squared error** 
$$
MSE=\frac{1}{n}\sum_{i=1}^n(y_i-\hat{y})^2
$$
is as small as possible given *a priori* assumptions on the form of $f(x)$, such as being a linear function $f(x)=\beta_0+\beta_1 x$. 

Regression is the starting point for linear regression, ANOVA and several other classical techniques.

## Linear Regression

Linear regression posits a relationship between two numerical random variables $X$ and $Y$ of the form
$$
Y \sim N(\beta_0+\beta_1 X, \sigma^2).
$$
The parameters $\beta_0$ (the **intercept**), $\beta_1$ (the **slope**) and $\sigma^2$ (the **variance**) are estimated from observations of $X$ and $Y$. Specifically, writing $\bar x = \sum^n_{i=1}x_i$, and $\bar y = \sum^n_{i=1}y_i$ for the sample means and using a little calculus,
$$
\hat \beta_1 = \frac{\sum^n_{i=1}(x_i-\bar x)(y_i-\bar y)}{\sum^n_{i=1}(x_i-\bar x)^2}, \quad
\hat \beta_0 = \bar y - \hat\beta_1\bar x.
$$
The variance of the **residuals** $y_i-\hat{y}$ gives the estimate of $\sigma^2$.

## The data linear regression expects

Estimates from these formulas are expected to be accurate when $X$ is uniformly distributed and $Y$ is normally distributed about a linear function of $X$.
\scriptsize
```{r, fig.width=8, fig.height=3.5}
set.seed(5)
x <- runif(100)
y <- rnorm(100, mean = 1 + 2*x, sd = 0.5)
plot(y~x)
abline(1,2, col="blue")
```

## The `lm` function in R

\scriptsize
```{r, fig.width=8, fig.height=3.5}
lm1 <- lm(y~x)
lm1$coefficients
plot(y~x); abline(1,2, col="blue"); abline(lm1, col="red")
```

## Evaluating a linear model

\scriptsize
```{r}
summary(lm1)
```

## Check residuals

Residuals appear normally distributed around 0 with a consistent variance independent of $X$.
\scriptsize
```{r, fig.width=8, fig.height=3.5}
plot(lm1$residuals~x); abline(0,0, lty="dashed")
```

\normalsize
We'll discuss various ways to check homogeneity of variance. There's no one correct way to do it.

## Overall accuracy of the model

$R^2$ measures how much the variance in $Y$ is described by the model.

$$
R^2 = 1 - \frac{\sigma_\text{residuals}^2}{\sigma_y^2} = 1 - \frac{\sum_{i=1}^n(y_i-\hat y)^2}{\sum_{i=1}^n(y_i-\bar y)^2}
$$
```{r, echo=FALSE, fig.width=8, fig.height=3.5}
par(mfrow=c(1,2))
plot(y~x); abline(mean(y),0, lty="dashed"); segments(x,y,x,mean(y), col="deepskyblue")
plot(y~x); abline(lm1, col="red"); segments(x,y,x,lm1$fitted.values, col = "deepskyblue")
par(mfrow=c(1,1))
```
We'll talk about Adjusted $R^2$ and the $F$-statistic shortly.

## Accuracy of the coefficient estimates

The standard error of an estimator reflects how it varies under repeated sampling.
$$
SE(\hat\beta_1)=\sqrt{\frac{\sigma^2}{\sum_{i=1}^n(x_i-\bar x)^2}}, \quad SE(\hat\beta_0)=\sqrt{\frac{\sigma^2}{n}+\frac{\sigma^2\bar x^2}{\sum_{i=1}^n(x_i-\bar x^2)}}
$$

* Standard errors allow us to compute confidence intervals for these parameters.
* For samples such as ours, there is a 95% chance that the interval 
$$
\hat\beta_1 \pm 2 \cdot SE(\hat\beta_1)
$$
contains the true value of $\beta_1$. (Which is 2.) 
* In our case, the 95% confidence interval is
$$
2.076 \pm 2 \cdot 0.161 = (1.754,2.399)
$$

## Simulate taking 5000 such samples and calculating $\beta_1$

\scriptsize
```{r, fig.width=8, fig.height=3}
betas <- numeric(5000); b_captured <- logical(5000)
for(i in 1:5000){
 x <- runif(100); y <- rnorm(100, mean = 1 + 2*x, sd = 0.5)
 lmi <- lm(y~x); slope <- coef(summary(lmi))[2,]
 betas[i] <- slope[1]
 b_captured[i] <- (2>slope[1]-1.96*slope[2]) & (2<slope[1]+1.96*slope[2]) 
}
mean(b_captured); hist(betas)
```

## Hypothesis testing

Having the standard errors for the estimated parameters also allows us to do hypothesis tests. 

* Generally we are not interested in testing the intercept.
* Testing the null hypothesis
$$
H_0: \beta_1=0
$$
is equivalent to testing for no relationship between $X$ and $Y$.
* The $t$ statistic for this test is 
$$
t=\frac{\hat\beta_1}{SE(\hat\beta_1)}.
$$
* The null hypothesis distribution is a $t$ distribution with $n-2$ degrees of freedom. 

## Multiple Linear Regression

The theory is similar if there are multiple predictor variables.
$$
Y \sim N(\beta_0+\beta_1X_1+\beta_2X_2+\cdots+\beta_pX_p, \sigma^2)
$$

* The parameter $\beta_j$ is the average effect on $Y$ of a one unit
increase in $X_j$, holding all other predictors fixed.
* Ideally the predictors are uncorrelated --- this is called a **balanced design**.
    * Each coefficient can be estimated and tested separately.
    * Interpreting $\beta_j$ as above is possible.
* Correlations among predictors cause problems:
    * The variance of all coefficients tends to increase.
    * Interpretations become hazardous.
    
## Overall accuracy of the model revisited

$R^2$ is defined exactly as for one variable linear models. 

Write $SS_{tot}=\sum_{i=1}^n(y_i-\bar y)^2$ and $SS_{res}=\sum_{i=1}^n(y_i-\hat y)^2$:
$$
R^2 = 1 - \frac{SS_{res}}{SS_{tot}}=\frac{SS_{tot}-SS_{res}}{SS_{tot}}.
$$

* An immediate problem is that even a predictor $X_j$ that has *nothing* to do with $Y$ is going to give *some* reduction in $R^2$, because $\hat\beta_j$ will not be *exactly* zero. 

## Two alternatives to $R^2$

* Adjust $R^2$ to penalize models for having more predictors:
$$
\text{Adjusted } R^2 = 1 - \frac{SS_{res}/(n-p-1)}{SS_{tot}/(n-1)}
$$
    * This is still (loosely) interpretable as the proportion of the variance in the response explained by the model.
    * Nearly identical to $R^2$ when $n>>p$. 

* Another alternative is the $F$ statistic.
$$
F = \frac{(SS_{tot}-SS_{res})/p}{SS_{res}/(n-p-1)}
$$
    * This will be distributed as $F_{p,n-p-1}$ if **all** of the $\beta_j$ (except possibly $\beta_0$) are zero, so can be used to test
    \begin{align*}
    H_0 &: \beta_1=\beta_2=\cdots=\beta_p=0 \\
    H_a &: \text{At least one of the $\beta_j$ is non-zero.}
    \end{align*}
    
## Multiple regression with `lm`

Lily data from @lily.
\scriptsize
```{r}
lm2 <- lm(flowers~vegetative+gopher+moisture, data=Lily_sum) # data in emdbook
summary(lm2)
```

## Pairs plot of the Lily data

\scriptsize
```{r, fig.width=8, fig.height=5}
pairs(Lily_sum[,c("flowers", "vegetative", "gopher", "moisture")])
```

## Confounders can have big effects
The Lily data contain an additional variable.
\scriptsize
```{r, fig.width=8, fig.height=2.5}
lm3 <- lm(flowers~vegetative+gopher+moisture+rockiness, data=Lily_sum)
summary(lm3)$coefficients; summary(lm3)$adj.r.squared

ggplot(Lily_sum, aes(moisture, flowers, color=rockiness))+
  geom_jitter(height = 0)
```

## Interactions
To include an interaction term use `*` in the formula.
\scriptsize
```{r, fig.width=8, fig.height=2.5}
lm4 <- lm(flowers~vegetative+gopher+moisture*rockiness, data=Lily_sum)
summary(lm4)$coefficients; summary(lm4)$adj.r.squared
```

\normalsize

The resulting model has a term for the product of the interacting variables.
$$
\widehat{\text{flw}}=4.51+0.73\text{veg}-1.07\text{gph}+2.98\text{mst}+0.24\text{rck}-0.02(\text{mst}\times\text{rck})
$$
Or, alternatively
$$
\widehat{\text{flw}}=4.51+0.73\text{veg}-1.07\text{gph}+(2.98-0.02\text{rck})\text{mst}+0.24\text{rck}
$$

## Categorical predictors

It is common for some or all of the predictor variables in a regression to be categorical.

* Presence/ Absence
* Treatment levels: (low, medium, high)
* Species

Categorical variables are encoded for regression using **indicator (dummy) variables**.

Example: $X$ is a categorical variable with levels $a$, $b$, $c$. Arbitrarily choose $a$ as the **reference level** and define
$$
Z_b=\begin{cases} 
0 & \mbox{if } X =a\text{ or }c \\
1 & \mbox{if } X=b 
\end{cases}
\qquad
Z_c=\begin{cases} 
0 & \mbox{if } X = a \text{ or } b\\
1 & \mbox{if } X=c 
\end{cases}
$$

## One way ANOVA

Continuing with the above example. Given data, regression will estimate the parameters for a model
$$
Y \sim N(\beta_0+\beta_1Z_b+\beta_2Z_c, \sigma^2).
$$

* If $X=a$, the model predicts $\hat y= \hat\beta_0$.
* If $X=b$, the model predicts $\hat y= \hat\beta_0 + \hat\beta_1$.
* If $X=c$, the model predicts $\hat y= \hat\beta_0 + \hat\beta_2$.

The hypothesis test using the $F$ statistic as above to test
$$
H_0: \beta_1=\beta_2=0, \quad H_a: \text{At least one of $\beta_1$ or $\beta_2$ is non-zero}
$$
is what is classically called **analysis of variance** or ANOVA.


## Multi-way ANOVA

Regression with multilple categorical predictors is called **multi-way** or **multi-factor anova**. 

[Tadpole data](https://sakai.unc.edu/access/content/group/3d1eb92e-7848-4f55-90c3-7c72a54e7e43/public/docs/lectures/lecture2.htm#description) acquired from @weiss. Note unequal variances.
\scriptsize
```{r, fig.width=8, fig.height=3.5, warning=FALSE}
tadpoles <- read.csv("data/tadpoles.csv")
tadpoles$fac3 <- as.factor(tadpoles$fac3) # It's coded as 1 or 2.
ggplot(tadpoles, aes(fac3, response, fill = fac2))+
  geom_boxplot()+facet_wrap(~fac1)+
  scale_fill_brewer(palette = "Dark2")    # The default colors get boring.
```


## A more general $F$ statistic

Earlier we used $F$ to compare a model to the **null model**, with no predictors.

A more general $F$ statistic can compare any two models where one is an extension of the other by adding predictors. To quantify the advantage of a new model obtained by adding variables to an existing model, compute 
$$
F = \frac{(SS_{old}-SS_{new})/(\text{number of new parameters} )}{SS_{new}/(\text{number of data points less parameters})}.
$$

* The number of new parameters, called the **numerator degrees of freedom** is the count of additional indicator variables in the extended model.
* The **denominator degrees of freedom** is the number of data points minus the total number of parameters in the extended model.


## ANOVA tables

The `anova` command calculates $F$ statistics to compare models.

\scriptsize
```{r}
lm5 <- lm(response~fac1*fac2*fac3, data = tadpoles)
anova(lm5) # summary is not as useful as it analyzes indicator variables
```
\normalsize
It appears that `fac1`, `fac2` and `fac3` all have significant effects on the response, as do the interactions `fac1:fac2` and `fac2:fac3`.

## The model suggested by the ANOVA

Now we can build a model using only those terms listed as significant.
\scriptsize
```{r}
lm5a <- lm(response~fac1+fac2+fac3+fac1:fac2+fac2:fac3, data = tadpoles)
summary(lm5a)$coefficients
```
\normalsize
Note that the coefficients on `fac2S` and `fac1No:fac2S` are not significant, it is only in its interactions with `fac1Ru` and `fac32` that the diet factor appears to have an effect. We will still include these terms in the model, because if we include an interaction effect, we also include the corresponding main effects, and if we include an effect from one level of a factor, we include all levels.

## Interpreting the result
Plot the coefficients with confidence intervals.
\tiny
```{r fig.width=8, fig.height=2}
ests <- coef(lm5a)[-1] # The reference level is not of immediate interest.  
tad_model <- data.frame(var.labels=factor(names(ests), levels=names(ests)), ests, 
                        low95 = confint(lm5a)[-1,1], up95 = confint(lm5a)[-1,2])
ggplot(tad_model, aes(var.labels, ests))+
  geom_pointrange(aes(ymin=low95, ymax=up95))+
  geom_hline(yintercept=0, linetype = "dashed", color = "red")+
  labs(x = "Term", y =  "Effect")+ coord_flip()
```
\normalsize
The reference treatment is `CoD1`. 

* `Ru` and `No` differ from `Co` but not each other.
* On its own, Diet does not have a significant effect.
* Sibships 1 and 2 have different mitotic levels.
* Shrimp in combination with `Ru` or sibship 2 has an effect.

## Type I (Sequential) and Type II (Marginal) ANOVA

**Type I** anova **sequential**ly adds each term in a list to a model containing the terms before it on that list.

**Type II** or **marginal** anova compares a model to the model including all possible other terms. 

Often, type II is preferred. For example, why evaluate `fac1` against the null model, `fac2` against `fac1`, and `fac3` against `fac1` and `fac2` if the order in which they are labelled is arbitrary? 

Additionally, type II anova is more robust to deviations from the assumption of equal group sizes, resulting in unbalanced designs.

## Marginal ANOVA using the `car` package

The base R command `anova` does sequential anova. Marginal anova is done using the `Anova` command in the `car` package.

\scriptsize
```{r}
Anova(lm5)
```
\normalsize
Results are similar to the type I analysis, but the p values for `fac1:fac2` are on opposite sides of the bright line of $0.05$.

## Combining numerical and categorical predictors

Often we have both numerical and categorical predictors. 

Seed production example from @crawley [p. 538].

\scriptsize
```{r, fig.width=8, fig.height=3.5}
ipo <- read.csv('data/ipomopsis.csv')
ggplot(data=ipo, aes(x=Root, y=Fruit, color = Grazing))+
  geom_point()
```

## ANCOVA

Does the categorical predictor `Grazing` effect the numerical response `Fruit`? The numerical variable `Root` is a confounder. This is classical **analysis of covariance** or ANCOVA. Once again, it's just regression.

\scriptsize
```{r}
lm6<- lm(Fruit~Root*Grazing, data=ipo)
anova(lm6) # sequential and marginal are identical in this case
```
\normalsize

* `Root` and `Fruit` appear correlated.
* `Grazing` and `Fruit` appear correlated.
* The interaction between `Grazing` and `Root` does not appear to affect `Fruit`.

## Homogeneity of Variance

Regression assumes that the variance in the residuals is constant for all values of the predictors, or **homoscedasticity**. For real data, we must asses the variance in the residuals against each predictor. 

In the tadpole data, the `CoD1`, `CoD2` and `RuD1` treatment groups have higher variances than the others. Is this a problem?
\scriptsize
```{r, fig.width=6, fig.height=1.5, warning=FALSE}
ggplot(tadpoles, aes(treatment, response))+
  stat_summary(fun=var, geom="point", shape = 23) # Show variances.
```
\normalsize

* For the `CoD1` and `CoD2` groups, the variance is due to outliers. Run the analysis without them - does the result change?
* `RuD1` might be a problem, but it is only one group, and while the variance is large, at least there isn't much skew.

## Independence of observations

Regression assumes that observations are independent, but this is often violated for real data.

**Pseudoreplication** is the technical term for data that includes dependent observations. It has the effect of artificially increasing the power of statistical tests. There are two very common scenarios where it is encountered:

* **Repeated measures**: Observe the same individual multiple times.
* **Block designs** and **Split plots**: Values of one variable are constant for grouped sets of observations.

We will discuss solutions to these issues later in the course.

## References

