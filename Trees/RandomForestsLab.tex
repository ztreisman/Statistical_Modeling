% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\title{Classification, Decision Trees and Random Forests}
\author{}
\date{\vspace{-2.5em}}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Classification, Decision Trees and Random Forests},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\begin{document}
\maketitle

\hypertarget{setup}{%
\section{Setup}\label{setup}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Attaching package: 'dplyr'
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:stats':
## 
##     filter, lag
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:base':
## 
##     intersect, setdiff, setequal, union
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(nnet)}
\FunctionTok{library}\NormalTok{(MASS)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Attaching package: 'MASS'
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:dplyr':
## 
##     select
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tree)}
\FunctionTok{library}\NormalTok{(randomForest)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## randomForest 4.6-14
\end{verbatim}

\begin{verbatim}
## Type rfNews() to see new features/changes/bug fixes.
\end{verbatim}

\begin{verbatim}
## 
## Attaching package: 'randomForest'
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:dplyr':
## 
##     combine
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Attaching package: 'ggplot2'
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:randomForest':
## 
##     margin
\end{verbatim}

We will work with the \texttt{pierce\_county\_house\_sales} data that
was referenced in the exam. These data are avaialble here: Our goal will
be to answer the obvious (and difficult) question: What makes a house
cost more or less?

Once you have the data downloaded and saved in the folder you want, they
can be loaded with an appropriate modification of the following command.
(I put the file in a subdirectory of my working directory called
\texttt{data}.)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{load}\NormalTok{(}\StringTok{"data/pierce\_county\_house\_sales.rda"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Take a look at the data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#View(pierce\_county\_house\_sales)}
\FunctionTok{summary}\NormalTok{(pierce\_county\_house\_sales)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    sale_date            sale_price      house_square_feet
##  Min.   :2020-01-01   Min.   :   2000   Min.   :   1     
##  1st Qu.:2020-04-27   1st Qu.: 348000   1st Qu.:1320     
##  Median :2020-07-27   Median : 416500   Median :1774     
##  Mean   :2020-07-16   Mean   : 461233   Mean   :1880     
##  3rd Qu.:2020-10-09   3rd Qu.: 523806   3rd Qu.:2352     
##  Max.   :2020-12-31   Max.   :6130000   Max.   :9510     
##  attic_finished_square_feet basement_square_feet attached_garage_square_feet
##  Min.   :   0.00            Min.   :   0.0       Min.   :   0.0             
##  1st Qu.:   0.00            1st Qu.:   0.0       1st Qu.:   0.0             
##  Median :   0.00            Median :   0.0       Median : 420.0             
##  Mean   :  24.95            Mean   : 167.9       Mean   : 364.4             
##  3rd Qu.:   0.00            3rd Qu.:   0.0       3rd Qu.: 528.0             
##  Max.   :1212.00            Max.   :4000.0       Max.   :2816.0             
##  detached_garage_square_feet   fireplaces    hvac_description  
##  Min.   :   0.0              Min.   :0.000   Length:16814      
##  1st Qu.:   0.0              1st Qu.:1.000   Class :character  
##  Median :   0.0              Median :1.000   Mode  :character  
##  Mean   :  38.3              Mean   :0.889                     
##  3rd Qu.:   0.0              3rd Qu.:1.000                     
##  Max.   :3664.0              Max.   :5.000                     
##    exterior           interior            stories       roof_cover       
##  Length:16814       Length:16814       Min.   :0.000   Length:16814      
##  Class :character   Class :character   1st Qu.:1.000   Class :character  
##  Mode  :character   Mode  :character   Median :2.000   Mode  :character  
##                                        Mean   :1.558                     
##                                        3rd Qu.:2.000                     
##                                        Max.   :3.000                     
##    year_built      bedrooms        bathrooms     waterfront_type   
##  Min.   :1880   Min.   : 0.000   Min.   :0.000   Length:16814      
##  1st Qu.:1959   1st Qu.: 3.000   1st Qu.:2.000   Class :character  
##  Median :1990   Median : 3.000   Median :2.000   Mode  :character  
##  Mean   :1980   Mean   : 3.278   Mean   :2.317                     
##  3rd Qu.:2006   3rd Qu.: 4.000   3rd Qu.:3.000                     
##  Max.   :2021   Max.   :25.000   Max.   :8.000                     
##  view_quality       utility_sewer     
##  Length:16814       Length:16814      
##  Class :character   Class :character  
##  Mode  :character   Mode  :character  
##                                       
##                                       
## 
\end{verbatim}

The data need a little preparation for our purposes. First, we want to
make the \emph{character} variables into \emph{factor} variables. Many
modeling tools do this automatically, but not all. We could change each
variable one at a time, but the \texttt{mutate} and \texttt{across}
commands allow us to adjust all seven of these variables at once.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pierce\_county\_house\_sales }\OtherTok{\textless{}{-}}\NormalTok{ pierce\_county\_house\_sales }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}
  \FunctionTok{across}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"hvac\_description"}\NormalTok{,  }\StringTok{"exterior"}\NormalTok{, }\StringTok{"interior"}\NormalTok{, }\StringTok{"roof\_cover"}\NormalTok{, }
           \StringTok{"waterfront\_type"}\NormalTok{, }\StringTok{"view\_quality"}\NormalTok{, }\StringTok{"utility\_sewer"}\NormalTok{), }
\NormalTok{         as.factor))}
\end{Highlighting}
\end{Shaded}

Second, we want a categorical response variable to practice our
classification tools. We'll build one out of \texttt{sale\_price}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pierce\_county\_house\_sales}\SpecialCharTok{$}\NormalTok{price\_category }\OtherTok{\textless{}{-}} \FunctionTok{cut}\NormalTok{(pierce\_county\_house\_sales}\SpecialCharTok{$}\NormalTok{sale\_price, }
                       \AttributeTok{breaks =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{300000}\NormalTok{, }\DecValTok{400000}\NormalTok{, }\DecValTok{500000}\NormalTok{, }\DecValTok{1000000}\NormalTok{, }\ConstantTok{Inf}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Run \texttt{summary} again and notice what has changed.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  Does the \texttt{price\_category} variable seem to capture essential
  information about \texttt{sale\_price}? In what ways might it be
  preferable to use this categorical variable instead of the numeric
  one? In what ways is the numeric response preferable?
\end{enumerate}

\hypertarget{classification}{%
\section{Classification}\label{classification}}

Now we want to use the other variables to try to determine the price
category for one of these houses. We'll avoid using \texttt{sale\_price}
since that would be cheating. The \texttt{sale\_date} variable causes
problems for some of our tools so we'll sometimes avoid that as well.

\hypertarget{multinomial-regression}{%
\subsection{Multinomial Regression}\label{multinomial-regression}}

Let's try multinomial regression first. The following code builds the
model, and then builds a table, called a \textbf{confusion matrix}, that
compares the predicted category to the actual category.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mlr.house }\OtherTok{\textless{}{-}} \FunctionTok{multinom}\NormalTok{(price\_category}\SpecialCharTok{\textasciitilde{}}\NormalTok{.}\SpecialCharTok{{-}}\NormalTok{sale\_price, }\AttributeTok{data=}\NormalTok{pierce\_county\_house\_sales)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # weights:  310 (244 variable)
## initial  value 27061.089060 
## iter  10 value 19950.060478
## iter  20 value 19748.941848
## iter  30 value 19258.733737
## iter  40 value 18520.246421
## iter  50 value 17820.242445
## iter  60 value 17000.876238
## iter  70 value 16487.566199
## iter  80 value 16253.064320
## iter  90 value 16146.278171
## iter 100 value 16061.501772
## final  value 16061.501772 
## stopped after 100 iterations
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mlr.pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(mlr.house)}
\FunctionTok{table}\NormalTok{(mlr.pred, pierce\_county\_house\_sales}\SpecialCharTok{$}\NormalTok{price\_category)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                
## mlr.pred        (0,3e+05] (3e+05,4e+05] (4e+05,5e+05] (5e+05,1e+06] (1e+06,Inf]
##   (0,3e+05]           751           399            78            61           6
##   (3e+05,4e+05]      1205          3849          1340           250          11
##   (4e+05,5e+05]       169           907          2184           861           4
##   (5e+05,1e+06]       133           176           854          3183         163
##   (1e+06,Inf]           0             1             4            66         159
\end{verbatim}

To assess the accuracy, we can add the entries on the diagonal, and see
what fraction of the total houses were correctly categorized.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(mlr.pred}\SpecialCharTok{==}\NormalTok{pierce\_county\_house\_sales}\SpecialCharTok{$}\NormalTok{price\_category)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.6022362
\end{verbatim}

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{1}
\item
  What percentage of houses in the \(\$300K-\$400K\) range were
  correctly categorized?
\item
  Some of the houses that sold for over a million dollars were seriously
  miscategorized. Can you find any reasons why this might be the case?
  (This will require some digging into the data.)
\end{enumerate}

\hypertarget{discriminant-analysis}{%
\subsection{Discriminant analysis}\label{discriminant-analysis}}

We can run linear discriminant analysis and see how it does with these
data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lda.house }\OtherTok{\textless{}{-}} \FunctionTok{lda}\NormalTok{(price\_category}\SpecialCharTok{\textasciitilde{}}\NormalTok{.}\SpecialCharTok{{-}}\NormalTok{sale\_price, }\AttributeTok{data =}\NormalTok{ pierce\_county\_house\_sales)}
\NormalTok{lda.pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(lda.house)}
\FunctionTok{mean}\NormalTok{(lda.pred}\SpecialCharTok{$}\NormalTok{class}\SpecialCharTok{==}\NormalTok{pierce\_county\_house\_sales}\SpecialCharTok{$}\NormalTok{price\_category)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5834424
\end{verbatim}

One nice thing about linear discriminant analysis is that the
discriminants define a basis for a vector space that should do a
reasonably good job of showing the difference between the categories.
We'll talk more about this when we discuss Principal Components Analysis
and other related tools. For now, we'll just use the output.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lda.obs }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(lda.pred}\SpecialCharTok{$}\NormalTok{x)}
\FunctionTok{ggplot}\NormalTok{(lda.obs, }
       \FunctionTok{aes}\NormalTok{(LD1, LD2, }
           \AttributeTok{color=}\NormalTok{pierce\_county\_house\_sales}\SpecialCharTok{$}\NormalTok{price\_category, }
           \AttributeTok{shape=}\NormalTok{lda.pred}\SpecialCharTok{$}\NormalTok{class)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{RandomForestsLab_files/figure-latex/unnamed-chunk-9-1.pdf}

\hypertarget{trees-for-classification}{%
\subsection{Trees for classification}\label{trees-for-classification}}

Trees are nice and intuitive. Let's build one.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tree.house }\OtherTok{\textless{}{-}} \FunctionTok{tree}\NormalTok{(price\_category}\SpecialCharTok{\textasciitilde{}}\NormalTok{.}\SpecialCharTok{{-}}\NormalTok{sale\_price}\SpecialCharTok{{-}}\NormalTok{sale\_date, pierce\_county\_house\_sales)}
\FunctionTok{summary}\NormalTok{(tree.house)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Classification tree:
## tree(formula = price_category ~ . - sale_price - sale_date, data = pierce_county_house_sales)
## Variables actually used in tree construction:
## [1] "house_square_feet"    "bathrooms"            "basement_square_feet"
## Number of terminal nodes:  7 
## Residual mean deviance:  2.188 = 36770 / 16810 
## Misclassification error rate: 0.4767 = 8015 / 16814
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(tree.house)}
\FunctionTok{text}\NormalTok{(tree.house) }
\end{Highlighting}
\end{Shaded}

\includegraphics{RandomForestsLab_files/figure-latex/unnamed-chunk-10-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tree.house }\CommentTok{\# Note the legend at the top of the output.}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## node), split, n, deviance, yval, (yprob)
##       * denotes terminal node
## 
##  1) root 16814 47630.0 (3e+05,4e+05] ( 0.1342929 0.3171167 0.2652551 0.2629356 0.0203997 )  
##    2) house_square_feet < 1918.5 9687 24770.0 (3e+05,4e+05] ( 0.2131723 0.4693920 0.2102818 0.0998245 0.0073294 )  
##      4) bathrooms < 1.5 2924  6437.0 (0,3e+05] ( 0.4473324 0.4227086 0.0790014 0.0413817 0.0095759 ) *
##      5) bathrooms > 1.5 6763 16770.0 (3e+05,4e+05] ( 0.1119326 0.4895756 0.2670413 0.1250924 0.0063581 )  
##       10) basement_square_feet < 465 5544 12510.0 (3e+05,4e+05] ( 0.1165224 0.5461760 0.2647908 0.0714286 0.0010823 )  
##         20) house_square_feet < 1618.5 3262  6639.0 (3e+05,4e+05] ( 0.1563458 0.6290619 0.1771919 0.0358676 0.0015328 ) *
##         21) house_square_feet > 1618.5 2282  5289.0 (3e+05,4e+05] ( 0.0595968 0.4276950 0.3900088 0.1222612 0.0004382 ) *
##       11) basement_square_feet > 465 1219  3381.0 (5e+05,1e+06] ( 0.0910582 0.2321575 0.2772765 0.3691550 0.0303527 ) *
##    3) house_square_feet > 1918.5 7127 16870.0 (5e+05,1e+06] ( 0.0270801 0.1101445 0.3399747 0.4846359 0.0381647 )  
##      6) house_square_feet < 2715.5 4714 11310.0 (4e+05,5e+05] ( 0.0313958 0.1588884 0.4270259 0.3663555 0.0163343 )  
##       12) basement_square_feet < 566 4229  9788.0 (4e+05,5e+05] ( 0.0321589 0.1709624 0.4625207 0.3303381 0.0040199 ) *
##       13) basement_square_feet > 566 485   989.9 (5e+05,1e+06] ( 0.0247423 0.0536082 0.1175258 0.6804124 0.1237113 ) *
##      7) house_square_feet > 2715.5 2413  4251.0 (5e+05,1e+06] ( 0.0186490 0.0149192 0.1699130 0.7157066 0.0808123 ) *
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tree.pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(tree.house, }\AttributeTok{type=}\StringTok{"class"}\NormalTok{)}
\FunctionTok{mean}\NormalTok{(tree.pred}\SpecialCharTok{==}\NormalTok{pierce\_county\_house\_sales}\SpecialCharTok{$}\NormalTok{price\_category)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5233139
\end{verbatim}

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{3}
\tightlist
\item
  According to classification accuracy, how do the three models we have
  seen so far compare?
\end{enumerate}

\hypertarget{cross-validation}{%
\section{Cross validation}\label{cross-validation}}

A problem with everything we have done so far to build and evaluate our
models is that we are computing \textbf{training error}. It is better
for assessing the predictive quality of a model to see how it does on
data that were not used in its construction. This is what we call
\textbf{test error}.

To evaluate test error, we need to hold back some of our data and not
use it to build the model. The following code pulls out 30\% of the data
to be used for testing and uses the remaining 70\% for training.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{train }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(pierce\_county\_house\_sales), }\FloatTok{0.7}\SpecialCharTok{*}\FunctionTok{nrow}\NormalTok{(pierce\_county\_house\_sales))}
\NormalTok{houses.test }\OtherTok{\textless{}{-}}\NormalTok{ pierce\_county\_house\_sales[}\SpecialCharTok{{-}}\NormalTok{train,]}
\NormalTok{tree.house }\OtherTok{\textless{}{-}} \FunctionTok{tree}\NormalTok{(price\_category}\SpecialCharTok{\textasciitilde{}}\NormalTok{.}\SpecialCharTok{{-}}\NormalTok{sale\_price}\SpecialCharTok{{-}}\NormalTok{sale\_date, pierce\_county\_house\_sales, }\AttributeTok{subset=}\NormalTok{train)}
\FunctionTok{plot}\NormalTok{(tree.house)}
\FunctionTok{text}\NormalTok{(tree.house)}
\end{Highlighting}
\end{Shaded}

\includegraphics{RandomForestsLab_files/figure-latex/unnamed-chunk-11-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tree.pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(tree.house, }\AttributeTok{newdata=}\NormalTok{houses.test, }\AttributeTok{type=}\StringTok{"class"}\NormalTok{)}
\NormalTok{confusion\_matrix }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(tree.pred,houses.test}\SpecialCharTok{$}\NormalTok{price\_category)}
\NormalTok{confusion\_matrix}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                
## tree.pred       (0,3e+05] (3e+05,4e+05] (4e+05,5e+05] (5e+05,1e+06] (1e+06,Inf]
##   (0,3e+05]           398           383            72            36           9
##   (3e+05,4e+05]       163           742           278            55           2
##   (4e+05,5e+05]        47           329           529           217           1
##   (5e+05,1e+06]        63           156           506           981          78
##   (1e+06,Inf]           0             0             0             0           0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(tree.pred}\SpecialCharTok{==}\NormalTok{houses.test}\SpecialCharTok{$}\NormalTok{price\_category)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5252725
\end{verbatim}

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{4}
\tightlist
\item
  Rebuild the other two models above and compute the test error instead
  of the training error. Does the ranking change?
\end{enumerate}

\hypertarget{bagging-and-random-forests}{%
\section{Bagging and Random Forests}\label{bagging-and-random-forests}}

Bootstrap aggregation, or bagging, is a general-purpose procedure for
reducing the variance of a statistical learning method; we introduce it
here because it is particularly useful and frequently used in the
context of decision trees.

Recall that given a set of \(n\) independent observations
\(Z_1 , \ldots , Z_n\) , each with variance \(\sigma^2\) , the variance
of the mean \(\bar{Z}\) of the observations is given by \(\sigma^2/n\).
In other words, averaging a set of observations reduces variance. Of
course, this is not practical because we generally do not have access to
multiple training sets. Instead, we can bootstrap, by taking repeated
samples from the (single) training data set.

In this approach we generate \(B\) different bootstrapped training data
sets. We then train our method on the \(b\)th bootstrapped training set
in order to get \(\hat f^{∗b}(x)\), the prediction at a point \(x\). If
the response is numeric, we then average all the predictions to obtain
\[
\hat  f_{bag}(x) = \frac{1}{B}\sum_{b=1}^B f^{∗b}(x)
\] If the response is categorical, then \(\hat f_{bag}(x)\) is the most
commonly occurring class among the \(\hat f^{∗b}(x)\). This is called
\textbf{bagging}.

\hypertarget{random-forest-using-all-predictors-is-bagging.}{%
\subsection{Random Forest using all predictors is
bagging.}\label{random-forest-using-all-predictors-is-bagging.}}

There are two tricks to the Random Forest algorithm. Bagging is one of
them.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bag.house }\OtherTok{\textless{}{-}} \FunctionTok{randomForest}\NormalTok{(price\_category}\SpecialCharTok{\textasciitilde{}}\NormalTok{.}\SpecialCharTok{{-}}\NormalTok{sale\_price,}
                          \AttributeTok{data=}\NormalTok{pierce\_county\_house\_sales,}
                       \AttributeTok{subset=}\NormalTok{train,}\AttributeTok{mtry=}\DecValTok{18}\NormalTok{,}\AttributeTok{importance=}\ConstantTok{TRUE}\NormalTok{)}
\NormalTok{bag.house}
\NormalTok{bag.pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(bag.house,}\AttributeTok{newdata =}\NormalTok{ houses.test)}
\FunctionTok{mean}\NormalTok{(bag.pred}\SpecialCharTok{==}\NormalTok{houses.test}\SpecialCharTok{$}\NormalTok{sale\_price)}
\end{Highlighting}
\end{Shaded}

\hypertarget{letting-the-less-obvious-predictors-have-their-turn}{%
\subsection{Letting the less obvious predictors have their
turn}\label{letting-the-less-obvious-predictors-have-their-turn}}

The other trick to a Random Forest is that at each step, not all of the
predictors are considered. The model is built by randomly ignoring all
but a subset of the predictors every time a split is considered.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rf.house }\OtherTok{\textless{}{-}} \FunctionTok{randomForest}\NormalTok{(price\_category}\SpecialCharTok{\textasciitilde{}}\NormalTok{.}\SpecialCharTok{{-}}\NormalTok{sale\_price,}\AttributeTok{data=}\NormalTok{pierce\_county\_house\_sales,}
                       \AttributeTok{subset=}\NormalTok{train,}\AttributeTok{importance=}\ConstantTok{TRUE}\NormalTok{)}
\NormalTok{rf.pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(rf.house,}\AttributeTok{newdata=}\NormalTok{houses.test)}
\FunctionTok{mean}\NormalTok{(rf.pred}\SpecialCharTok{==}\NormalTok{houses.test}\SpecialCharTok{$}\NormalTok{price\_category)}
\end{Highlighting}
\end{Shaded}

\hypertarget{interpreting-a-random-forest}{%
\subsection{Interpreting a Random
Forest}\label{interpreting-a-random-forest}}

The Random Forest algorithm produces lots of trees, and it can be hard
to interpret the result. One thing we can do is look at how often a
variable was used in building a tree and how much cumulative effect at
improving the error each predictor had.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{importance}\NormalTok{(rf.house)}
\FunctionTok{varImpPlot}\NormalTok{(rf.house)}
\end{Highlighting}
\end{Shaded}


\end{document}
